{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mwawann\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cosmic-shadow-84</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wawann/url_watcher\" target=\"_blank\">https://wandb.ai/wawann/url_watcher</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wawann/url_watcher/runs/3kat54rv\" target=\"_blank\">https://wandb.ai/wawann/url_watcher/runs/3kat54rv</a><br/>\n",
       "                Run data is saved locally in <code>/home/wawann/PycharmProjects/url_watcher/wandb/run-20210609_043021-3kat54rv</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(3kat54rv)</h1><iframe src=\"https://wandb.ai/wawann/url_watcher/runs/3kat54rv\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f97ae2dceb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import wandb\n",
    "\n",
    "old_dataset = \"data/filesplit2/\"\n",
    "old_testset = \"data/test/\"\n",
    "kaggle_set_folder = \"data/kaggle_set_folder\"\n",
    "kaggle_test_set_folder = \"data/kaggle_test_set_folder\"\n",
    "small_kaggle_set_folder = \"data/small_kaggle_set_folder\"\n",
    "small_kaggle_test_set_folder = \"data/small_kaggle_test_set_folder\"\n",
    "history_set_folder = \"data/history_set_folder\"\n",
    "history_test_set_folder = \"data/history_test_set_folder\"\n",
    "combined_set_folder = \"data/combined_set_folder\"\n",
    "combined_test_set_folder = \"data/combined_test_set_folder\"\n",
    "MODELS_FOLDER = \"IA_models\"\n",
    "\n",
    "MODEL_NAME = \"combined.h5\"\n",
    "MAX_FEATURES = 2000\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 15\n",
    "OUTPUT_DIM = 10\n",
    "TRAIN_SET = combined_set_folder\n",
    "TEST_SET = combined_test_set_folder\n",
    "\n",
    "SEQUENCE_LENGTH = 150\n",
    "SEED = 42\n",
    "\n",
    "config_defaults = {\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 0.01\n",
    "}\n",
    "wandb.init(project=\"url_watcher\", config=config_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71578 files belonging to 2 classes.\n",
      "Using 57263 files for training.\n"
     ]
    }
   ],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "raw_train_data = tf.keras.preprocessing.text_dataset_from_directory(TRAIN_SET, batch_size=wandb.config.batch_size,\n",
    "                                                                    validation_split=0.2, subset=\"training\",\n",
    "                                                                    label_mode=\"binary\", seed=SEED)\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=MAX_FEATURES,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")\n",
    "text_ds = raw_train_data.map(lambda x, y: x, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    config_defaults = {\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.01\n",
    "    }\n",
    "    wandb.init(project=\"url_watcher\", config=config_defaults)\n",
    "    wandb.config.epochs = EPOCHS\n",
    "    raw_train_data = tf.keras.preprocessing.text_dataset_from_directory(TRAIN_SET, batch_size=wandb.config.batch_size,\n",
    "                                                                        validation_split=0.2, subset=\"training\",\n",
    "                                                                        label_mode=\"binary\", seed=SEED)\n",
    "    raw_val_data = tf.keras.preprocessing.text_dataset_from_directory(TRAIN_SET, batch_size=wandb.config.batch_size,\n",
    "                                                                      validation_split=0.2, subset=\"validation\",\n",
    "                                                                      label_mode=\"binary\", seed=SEED)\n",
    "    raw_test_data = tf.keras.preprocessing.text_dataset_from_directory(TEST_SET, batch_size=wandb.config.batch_size)\n",
    "\n",
    "    vectorize_layer = TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=MAX_FEATURES,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=SEQUENCE_LENGTH,\n",
    "    )\n",
    "    text_ds = raw_train_data.map(lambda x, y: x, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    vectorize_layer.adapt(text_ds)\n",
    "\n",
    "    train_ds = raw_train_data.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = raw_val_data.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds = raw_test_data.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    #standard model\n",
    "    from tensorflow.keras import layers\n",
    "\n",
    "    config = wandb.config\n",
    "    config.learning_rate = 0.01\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Embedding(MAX_FEATURES + 1, output_dim=OUTPUT_DIM),\n",
    "        layers.Dense(1000),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(100),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1)])\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=wandb.config.learning_rate),\n",
    "                  metrics=tf.metrics.BinaryAccuracy(threshold=0.0))\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=wandb.config.epochs, callbacks=[WandbCallback(log_weights=True, )])\n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "    model.save(os.path.join(wandb.run.dir, f\"batch_{wandb.config.batch_size}_lr_{wandb.config.learning_rate}.h5\"))\n",
    "\n",
    "    print(\"Base Model\")\n",
    "    print(\"Loss: \", loss)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    wandb.log({'Test Error Rate': round((1 - accuracy) * 100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: k8e07x9n\n",
      "Sweep URL: https://wandb.ai/wawann/url_watcher/sweeps/k8e07x9n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: 226k9a1c with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 8\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlearning_rate: 0.01\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Ignored wandb.init() arg project when running a sweep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ethereal-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wawann/url_watcher\" target=\"_blank\">https://wandb.ai/wawann/url_watcher</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/wawann/url_watcher/sweeps/k8e07x9n\" target=\"_blank\">https://wandb.ai/wawann/url_watcher/sweeps/k8e07x9n</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/wawann/url_watcher/runs/226k9a1c\" target=\"_blank\">https://wandb.ai/wawann/url_watcher/runs/226k9a1c</a><br/>\n",
       "                Run data is saved locally in <code>/home/wawann/PycharmProjects/url_watcher/wandb/run-20210609_043028-226k9a1c</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71578 files belonging to 2 classes.\n",
      "Using 57263 files for training.\n",
      "Found 71578 files belonging to 2 classes.\n",
      "Using 14315 files for validation.\n",
      "Found 2490 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 10)          20010     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 1000)        11000     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 1000)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 131,211\n",
      "Trainable params: 131,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "7158/7158 [==============================] - 19s 3ms/step - loss: 0.2922 - binary_accuracy: 0.9015 - val_loss: 0.2678 - val_binary_accuracy: 0.8943\n",
      "Epoch 2/15\n",
      "7158/7158 [==============================] - 18s 3ms/step - loss: 0.2569 - binary_accuracy: 0.9108 - val_loss: 0.2361 - val_binary_accuracy: 0.9107\n",
      "Epoch 3/15\n",
      "7158/7158 [==============================] - 18s 3ms/step - loss: 0.2518 - binary_accuracy: 0.9127 - val_loss: 0.2566 - val_binary_accuracy: 0.8973\n",
      "Epoch 4/15\n",
      "7158/7158 [==============================] - 17s 2ms/step - loss: 0.2480 - binary_accuracy: 0.9134 - val_loss: 0.2122 - val_binary_accuracy: 0.9260\n",
      "Epoch 5/15\n",
      "7158/7158 [==============================] - 17s 2ms/step - loss: 0.2499 - binary_accuracy: 0.9127 - val_loss: 0.2478 - val_binary_accuracy: 0.9039\n",
      "Epoch 6/15\n",
      "7158/7158 [==============================] - 17s 2ms/step - loss: 0.2525 - binary_accuracy: 0.9143 - val_loss: 0.2339 - val_binary_accuracy: 0.9097\n",
      "Epoch 7/15\n",
      "7158/7158 [==============================] - 17s 2ms/step - loss: 0.2507 - binary_accuracy: 0.9133 - val_loss: 0.2268 - val_binary_accuracy: 0.9143\n",
      "Epoch 8/15\n",
      "7158/7158 [==============================] - 17s 2ms/step - loss: 0.2502 - binary_accuracy: 0.9126 - val_loss: 0.2319 - val_binary_accuracy: 0.9123\n",
      "Epoch 9/15\n",
      "7158/7158 [==============================] - 18s 2ms/step - loss: 0.2535 - binary_accuracy: 0.9114 - val_loss: 0.2505 - val_binary_accuracy: 0.9116\n",
      "Epoch 10/15\n",
      "7158/7158 [==============================] - 18s 2ms/step - loss: 0.2538 - binary_accuracy: 0.9130 - val_loss: 0.2494 - val_binary_accuracy: 0.9150\n",
      "Epoch 11/15\n",
      "7158/7158 [==============================] - 18s 3ms/step - loss: 0.2488 - binary_accuracy: 0.9142 - val_loss: 0.2498 - val_binary_accuracy: 0.9090\n",
      "Epoch 12/15\n",
      "7158/7158 [==============================] - 18s 2ms/step - loss: 0.2483 - binary_accuracy: 0.9145 - val_loss: 0.2367 - val_binary_accuracy: 0.9132\n",
      "Epoch 13/15\n",
      "7158/7158 [==============================] - 17s 2ms/step - loss: 0.2460 - binary_accuracy: 0.9138 - val_loss: 0.2394 - val_binary_accuracy: 0.9130\n",
      "Epoch 14/15\n",
      "7158/7158 [==============================] - 17s 2ms/step - loss: 0.2503 - binary_accuracy: 0.9137 - val_loss: 0.2348 - val_binary_accuracy: 0.9157\n",
      "Epoch 15/15\n",
      "7158/7158 [==============================] - 17s 2ms/step - loss: 0.2469 - binary_accuracy: 0.9146 - val_loss: 0.2363 - val_binary_accuracy: 0.9017\n",
      "312/312 [==============================] - 0s 1ms/step - loss: 0.1975 - binary_accuracy: 0.9341\n",
      "Base Model\n",
      "Loss:  0.19746963679790497\n",
      "Accuracy:  0.9341365694999695\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 150888<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1.54MB of 1.54MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/wawann/PycharmProjects/url_watcher/wandb/run-20210609_043028-226k9a1c/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/wawann/PycharmProjects/url_watcher/wandb/run-20210609_043028-226k9a1c/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>14</td></tr><tr><td>loss</td><td>0.24583</td></tr><tr><td>binary_accuracy</td><td>0.91419</td></tr><tr><td>val_loss</td><td>0.23631</td></tr><tr><td>val_binary_accuracy</td><td>0.90171</td></tr><tr><td>_runtime</td><td>270</td></tr><tr><td>_timestamp</td><td>1623206098</td></tr><tr><td>_step</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.2122</td></tr><tr><td>best_epoch</td><td>3</td></tr><tr><td>Test Error Rate</td><td>6.59</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>loss</td><td>█▃▂▂▂▂▂▂▂▃▁▁▁▂▁</td></tr><tr><td>binary_accuracy</td><td>▁▅▆▇▇█▆▅▅▆▆█▇▇▇</td></tr><tr><td>val_loss</td><td>█▄▇▁▅▄▃▃▆▆▆▄▄▄▄</td></tr><tr><td>val_binary_accuracy</td><td>▁▅▂█▃▄▅▅▅▆▄▅▅▆▃</td></tr><tr><td>_runtime</td><td>▁▂▂▃▃▄▄▄▅▅▆▇▇███</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▃▄▄▄▅▅▆▇▇███</td></tr><tr><td>_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>Test Error Rate</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">ethereal-sweep-1</strong>: <a href=\"https://wandb.ai/wawann/url_watcher/runs/226k9a1c\" target=\"_blank\">https://wandb.ai/wawann/url_watcher/runs/226k9a1c</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: 8apou7r7 with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 16\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlearning_rate: 0.0005\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Ignored wandb.init() arg project when running a sweep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">super-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wawann/url_watcher\" target=\"_blank\">https://wandb.ai/wawann/url_watcher</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/wawann/url_watcher/sweeps/k8e07x9n\" target=\"_blank\">https://wandb.ai/wawann/url_watcher/sweeps/k8e07x9n</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/wawann/url_watcher/runs/8apou7r7\" target=\"_blank\">https://wandb.ai/wawann/url_watcher/runs/8apou7r7</a><br/>\n",
       "                Run data is saved locally in <code>/home/wawann/PycharmProjects/url_watcher/wandb/run-20210609_043503-8apou7r7</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71578 files belonging to 2 classes.\n",
      "Using 57263 files for training.\n",
      "Found 71578 files belonging to 2 classes.\n",
      "Using 14315 files for validation.\n",
      "Found 2490 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 10)          20010     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 1000)        11000     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 1000)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 131,211\n",
      "Trainable params: 131,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2939 - binary_accuracy: 0.8944 - val_loss: 0.2167 - val_binary_accuracy: 0.9226\n",
      "Epoch 2/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2121 - binary_accuracy: 0.9237 - val_loss: 0.2141 - val_binary_accuracy: 0.9237\n",
      "Epoch 3/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2115 - binary_accuracy: 0.9238 - val_loss: 0.2119 - val_binary_accuracy: 0.9251\n",
      "Epoch 4/15\n",
      "3579/3579 [==============================] - 11s 3ms/step - loss: 0.2114 - binary_accuracy: 0.9243 - val_loss: 0.2109 - val_binary_accuracy: 0.9259\n",
      "Epoch 5/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2103 - binary_accuracy: 0.9241 - val_loss: 0.2110 - val_binary_accuracy: 0.9274\n",
      "Epoch 6/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2103 - binary_accuracy: 0.9244 - val_loss: 0.2113 - val_binary_accuracy: 0.9268\n",
      "Epoch 7/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2100 - binary_accuracy: 0.9245 - val_loss: 0.2117 - val_binary_accuracy: 0.9269\n",
      "Epoch 8/15\n",
      "3579/3579 [==============================] - 11s 3ms/step - loss: 0.2097 - binary_accuracy: 0.9244 - val_loss: 0.2118 - val_binary_accuracy: 0.9270\n",
      "Epoch 9/15\n",
      "3579/3579 [==============================] - 11s 3ms/step - loss: 0.2098 - binary_accuracy: 0.9241 - val_loss: 0.2121 - val_binary_accuracy: 0.9270\n",
      "Epoch 10/15\n",
      "3579/3579 [==============================] - 11s 3ms/step - loss: 0.2100 - binary_accuracy: 0.9240 - val_loss: 0.2131 - val_binary_accuracy: 0.9266\n",
      "Epoch 11/15\n",
      "3579/3579 [==============================] - 11s 3ms/step - loss: 0.2094 - binary_accuracy: 0.9242 - val_loss: 0.2133 - val_binary_accuracy: 0.9266\n",
      "Epoch 12/15\n",
      "3579/3579 [==============================] - 11s 3ms/step - loss: 0.2091 - binary_accuracy: 0.9246 - val_loss: 0.2129 - val_binary_accuracy: 0.9265\n",
      "Epoch 13/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2097 - binary_accuracy: 0.9244 - val_loss: 0.2127 - val_binary_accuracy: 0.9269\n",
      "Epoch 14/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2094 - binary_accuracy: 0.9245 - val_loss: 0.2133 - val_binary_accuracy: 0.9262\n",
      "Epoch 15/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2094 - binary_accuracy: 0.9246 - val_loss: 0.2128 - val_binary_accuracy: 0.9268\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.2737 - binary_accuracy: 0.8787\n",
      "Base Model\n",
      "Loss:  0.27365684509277344\n",
      "Accuracy:  0.8787148594856262\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 151353<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1.54MB of 1.54MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/wawann/PycharmProjects/url_watcher/wandb/run-20210609_043503-8apou7r7/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/wawann/PycharmProjects/url_watcher/wandb/run-20210609_043503-8apou7r7/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>14</td></tr><tr><td>loss</td><td>0.20918</td></tr><tr><td>binary_accuracy</td><td>0.9247</td></tr><tr><td>val_loss</td><td>0.21279</td></tr><tr><td>val_binary_accuracy</td><td>0.92679</td></tr><tr><td>_runtime</td><td>179</td></tr><tr><td>_timestamp</td><td>1623206282</td></tr><tr><td>_step</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.21089</td></tr><tr><td>best_epoch</td><td>3</td></tr><tr><td>Test Error Rate</td><td>12.13</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>binary_accuracy</td><td>▁▇▇█████▇██████</td></tr><tr><td>val_loss</td><td>█▅▂▁▁▂▂▂▂▄▄▃▃▄▃</td></tr><tr><td>val_binary_accuracy</td><td>▁▃▅▆█▇▇▇▇▇▇▇▇▆▇</td></tr><tr><td>_runtime</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>Test Error Rate</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">super-sweep-2</strong>: <a href=\"https://wandb.ai/wawann/url_watcher/runs/8apou7r7\" target=\"_blank\">https://wandb.ai/wawann/url_watcher/runs/8apou7r7</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: co7j1gc0 with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 16\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlearning_rate: 0.0005\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Ignored wandb.init() arg project when running a sweep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">mild-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wawann/url_watcher\" target=\"_blank\">https://wandb.ai/wawann/url_watcher</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/wawann/url_watcher/sweeps/k8e07x9n\" target=\"_blank\">https://wandb.ai/wawann/url_watcher/sweeps/k8e07x9n</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/wawann/url_watcher/runs/co7j1gc0\" target=\"_blank\">https://wandb.ai/wawann/url_watcher/runs/co7j1gc0</a><br/>\n",
       "                Run data is saved locally in <code>/home/wawann/PycharmProjects/url_watcher/wandb/run-20210609_043805-co7j1gc0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71578 files belonging to 2 classes.\n",
      "Using 57263 files for training.\n",
      "Found 71578 files belonging to 2 classes.\n",
      "Using 14315 files for validation.\n",
      "Found 2490 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 10)          20010     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 1000)        11000     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 1000)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 131,211\n",
      "Trainable params: 131,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2944 - binary_accuracy: 0.8959 - val_loss: 0.2160 - val_binary_accuracy: 0.9232\n",
      "Epoch 2/15\n",
      "3579/3579 [==============================] - 11s 3ms/step - loss: 0.2125 - binary_accuracy: 0.9236 - val_loss: 0.2144 - val_binary_accuracy: 0.9239\n",
      "Epoch 3/15\n",
      "3579/3579 [==============================] - 11s 3ms/step - loss: 0.2116 - binary_accuracy: 0.9235 - val_loss: 0.2116 - val_binary_accuracy: 0.9256\n",
      "Epoch 4/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2110 - binary_accuracy: 0.9239 - val_loss: 0.2110 - val_binary_accuracy: 0.9262\n",
      "Epoch 5/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2108 - binary_accuracy: 0.9244 - val_loss: 0.2109 - val_binary_accuracy: 0.9270\n",
      "Epoch 6/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2101 - binary_accuracy: 0.9242 - val_loss: 0.2114 - val_binary_accuracy: 0.9270\n",
      "Epoch 7/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2105 - binary_accuracy: 0.9242 - val_loss: 0.2119 - val_binary_accuracy: 0.9271\n",
      "Epoch 8/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2104 - binary_accuracy: 0.9244 - val_loss: 0.2127 - val_binary_accuracy: 0.9267\n",
      "Epoch 9/15\n",
      "3579/3579 [==============================] - 11s 3ms/step - loss: 0.2100 - binary_accuracy: 0.9238 - val_loss: 0.2121 - val_binary_accuracy: 0.9270\n",
      "Epoch 10/15\n",
      "3579/3579 [==============================] - 12s 3ms/step - loss: 0.2102 - binary_accuracy: 0.9243 - val_loss: 0.2130 - val_binary_accuracy: 0.9266\n",
      "Epoch 11/15\n",
      "1539/3579 [===========>..................] - ETA: 6s - loss: 0.2096 - binary_accuracy: 0.9260"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'early_terminate': {\n",
    "        'type': 'hyperband',\n",
    "        'min_iter': 5\n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'values': [8, 16, 32, 64, 128, 256]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"url_watcher\")\n",
    "wandb.agent(sweep_id, function=train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}